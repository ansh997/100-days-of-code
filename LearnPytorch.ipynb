{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "F884gKYjq0cJ"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNqljRkpSugivOcHbZDf91D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ansh997/100-days-of-code/blob/master/LearnPytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning PyTorch as framework not as an API\n",
        "\n",
        "This is a new philosphical way to learn `PyTorch`. `PyTorch` comes with its own paradigm (tensor-first computation), execution model(dynamic graphs), and idioms (modules, autograd etc.).\n",
        "\n",
        "### What is PyTorch?\n",
        "- `Paradigm`: Imperative, Tensor-Based, Dynamically Executed\n",
        "- `Use-Case DNA`: Built for research first --> flexibility over performance\n",
        "- `Mental Model`: Everything is a tensor; gradients are computed automatically; models are modular graphs of operations"
      ],
      "metadata": {
        "id": "DwPgWrUgpbSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Tensors Feel like home\n",
        "\n",
        "\n",
        "- [x] Create and manipulate tensors(shapes, dtype, devices)\n",
        "- [x] Perform Operations - `addition`, `broadcasting`, `.view()`, `.reshape()`\n",
        "- [x] Compare against Numpy to develop intuition `torch.from_numpy(), tensor.numpy()`"
      ],
      "metadata": {
        "id": "F884gKYjq0cJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and manipulate tensors(shapes, dtype, devices)\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "data = [[1, 2], [3, 4]]\n",
        "x_data = torch.tensor(data)\n",
        "print('Attributes of a Tensor: ', x_data.shape, x_data.dtype, x_data.device)\n",
        "# (torch.Size([2, 2]), torch.int64, device(type='cpu'))\n",
        "\n",
        "# can be created from numpy array\n",
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)\n",
        "print('Attributes of a Tensor: ', x_np.shape, x_np.dtype, x_np.device)\n",
        "# (torch.Size([2, 2]), torch.int64, device(type='cpu'))\n",
        "\n",
        "# can be created from another tensor\n",
        "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
        "\n",
        "# With random or constant values:\n",
        "# shape is a tuple of tensor dimensions.\n",
        "# In the functions below, it determines the dimensionality of the output tensor.\n",
        "\n",
        "shape = (2,3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ],
      "metadata": {
        "id": "hT9NElsRsSE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning about Operations tensors\n",
        "tensor = torch.rand(3,4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")\n",
        "\n",
        "# We move our tensor to the current accelerator if available\n",
        "# Each of these operations can be run on the CPU and\n",
        "# Accelerator such as CUDA, MPS, MTIA, or XPU.\n",
        "if torch.accelerator.is_available():\n",
        "    print('Accelerator is available')\n",
        "    tensor = tensor.to(torch.accelerator.current_accelerator())\n",
        "\n",
        "# By default, tensors are created on the CPU.\n",
        "# We need to explicitly move tensors to the accelerator using `.to` method\n",
        "# (after checking for accelerator availability).\n",
        "\n",
        "# Standard numpy like indexing and slicing\n",
        "tensor = torch.ones(4, 4)\n",
        "print(f\"First row: {tensor[0]}\")\n",
        "print(f\"First column: {tensor[:, 0]}\")\n",
        "print(f\"Last column: {tensor[..., -1]}\")\n",
        "tensor[:,1] = 0\n",
        "print(tensor)\n",
        "\n",
        "# Arithmetic Operation\n",
        "# This computes the matrix multiplication between two tensors.\n",
        "# y1, y2, y3 will have the same value\n",
        "# ``tensor.T`` returns the transpose of a tensor\n",
        "y1 = tensor @ tensor.T\n",
        "y2 = tensor.matmul(tensor.T)  # same as transpose in numpy\n",
        "\n",
        "y3 = torch.rand_like(y1)\n",
        "torch.matmul(tensor, tensor.T, out=y3)\n",
        "\n",
        "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
        "# QUE: Do element-wise product should have same dimensions?\n",
        "#      If not what happens with dimension is not same?\n",
        "z1 = tensor * tensor\n",
        "z2 = tensor.mul(tensor)\n",
        "\n",
        "z3 = torch.rand_like(tensor)\n",
        "torch.mul(tensor, tensor, out=z3)\n",
        "\n",
        "# Single-element tensors If you have a one-element tensor,\n",
        "# for example by aggregating all values of a tensor into one value,\n",
        "# you can convert it to a Python numerical value using item()\n",
        "\n",
        "agg = tensor.sum()\n",
        "agg_item = agg.item()\n",
        "print(agg_item, type(agg_item))\n",
        "\n",
        "# In-place operations Operations that store the result into the operand are called in-place. They are denoted by a _ suffix. For example: x.copy_(y), x.t_(), will change x.\n",
        "print(f\"{tensor} \\n\")\n",
        "tensor.add_(5)\n",
        "print(tensor)\n",
        "# Note: In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged."
      ],
      "metadata": {
        "id": "uNU6tF8-y7Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Operations - addition, broadcasting, .view(), .reshape()\n",
        "# Tensor Addition\n",
        "a = torch.tensor([[1, 2], [3, 4]])\n",
        "b = torch.tensor([[5, 6], [7, 8]])\n",
        "result = a + b\n",
        "print(result)\n",
        "\n",
        "# Broadcasting enables arithmetic operations between tensors of different shapes by automatically expanding the smaller tensor to match the shape of the larger one.\n",
        "a = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Shape: (2, 3)\n",
        "b = torch.tensor([10, 20, 30])           # Shape: (3,)\n",
        "result = a + b\n",
        "print(result)\n",
        "\n",
        "# .view() vs .reshape()\n",
        "# .view(): Returns a new tensor with the same data but a different shape. It requires the tensor to be contiguous in memory. If the tensor is not contiguous, you need to call .contiguous() before using .view().\n",
        "\n",
        "# .reshape(): Similar to .view(), but it can handle non-contiguous tensors by returning a copy if necessary. It's more flexible and safer to use when you're unsure about the tensor's memory layout.\n",
        "\n",
        "x = torch.arange(6)  # Tensor with shape (6,)\n",
        "print(\"Original tensor:\", x)\n",
        "\n",
        "# Using view\n",
        "y = x.view(2, 3)\n",
        "print(\"Reshaped with view:\", y)\n",
        "\n",
        "# Using reshape\n",
        "z = x.reshape(3, 2)\n",
        "print(\"Reshaped with reshape:\", z)"
      ],
      "metadata": {
        "id": "y_j3T2q1C1y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Feature                     | NumPy `ndarray`                     | PyTorch `Tensor`                  |\n",
        "| --------------------------- | ----------------------------------- | --------------------------------- |\n",
        "| Device support              | CPU only                            | CPU + GPU (CUDA, MPS)             |\n",
        "| Auto-grad (Differentiation) | ❌ Not supported                     | ✅ Via `requires_grad`             |\n",
        "| Shared memory conversion    | ✅ with `.from_numpy()` / `.numpy()` | ✅ Shared unless explicitly cloned |\n",
        "| Broadcasting                | ✅                                   | ✅ Compatible                      |\n",
        "| Data type strictness        | Slightly relaxed                    | Stricter (esp. floats vs ints)    |\n"
      ],
      "metadata": {
        "id": "KjdCjEFYQI6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lens 1: Paradigm - Tensors, Graphs, and Gradients\n",
        "\n",
        "- [x] Understand dynamic Computation graph (eager mode vs static)\n",
        "- [x] Explore how autograd works:\n",
        "    - [x] .requires_grad_()\n",
        "    - [x] .backward()\n",
        "    - [x] Inspect .grad on parameters\n",
        "- [x] Build a simple 2-layer neural net from scratch using only tensor operations"
      ],
      "metadata": {
        "id": "pcaZ6Oa7sSt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding dynamic computation graph in PyTorch\n",
        "\n",
        "- One of the most important mental models\n",
        "- Different from the static graphs used in other frameworks like tensorflow\n",
        "\n",
        "> Q. What is a Computation Graph?\n",
        "\n",
        "A. A computation graph is a directed graph where:\n",
        "**bold text**\n",
        "- Nodes are operations (like add, multiply, relu, etc.)\n",
        "\n",
        "- Edges are tensors flowing between those operations\n",
        "\n",
        "The graph is used to:\n",
        "\n",
        "- Track computations\n",
        "\n",
        "- Compute gradients during backpropagation\n",
        "\n",
        "| Feature                  | **Dynamic (Eager)** – PyTorch            | **Static** – TensorFlow 1.x            |\n",
        "| ------------------------ | ---------------------------------------- | -------------------------------------- |\n",
        "| Graph defined at         | **Run-time** (during forward pass)       | **Compile-time** (before running)      |\n",
        "| Flexibility              | Very high – native Python control flow   | Limited – needs explicit graph def     |\n",
        "| Debugging                | Easy – just print like Python            | Harder – must inspect graph            |\n",
        "| Performance optimization | Less by default, but can use TorchScript | High, since graph is optimized upfront |\n",
        "\n",
        "🧠 **Eager Execution (PyTorch’s Default)**:\n",
        "\n",
        "Every operation you write in PyTorch immediately executes and gets added to the autograd tape (if requires_grad=True).\n",
        "\n",
        "| Topic             | PyTorch Default             |\n",
        "| ----------------- | --------------------------- |\n",
        "| Graph Type        | **Dynamic / Eager**         |\n",
        "| Defined At        | Run-time                    |\n",
        "| Debugging         | Natural Python              |\n",
        "| Gradient Tracking | Automatic via `.backward()` |\n",
        "| Static Option     | Via `torch.jit.script()`    |\n"
      ],
      "metadata": {
        "id": "vvFLVezEl72l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automatic Differentiation with torch.autograd\n",
        "\n",
        "PyTorch has a built-in differentiation engine called `torch.autograd` to compute gradients for `backprop` algorithm. It supports automatic computation of gradient for any computational graph.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "x = torch.ones(5)  # input tensor\n",
        "y = torch.zeros(3)  # expected output\n",
        "w = torch.randn(5, 3, requires_grad=True)  # tunable\n",
        "b = torch.randn(3, requires_grad=True)  # tunable\n",
        "z = torch.matmul(x, w)+b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
        "```\n",
        "\n",
        "![PyTorch Computation Graph](https://docs.pytorch.org/tutorials/_images/comp-graph.png)\n",
        "\n",
        "> This object knows how to compute the function in the forward direction, and also how to compute its derivative during the backward propagation step. A reference to the backward propagation function is stored in grad_fn property of a tensor.\n",
        "\n",
        "### Computing Gradients\n",
        "\n",
        "To optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function with respect to parameters, namely, we need $\\frac{\\partial \\text{loss}}{\\partial w}$ and $\\frac{\\partial \\text{loss}}{\\partial b}$ under some fixed values of x and y. To compute those derivatives, we call loss.backward(), and then retrieve the values from w.grad and b.grad\n",
        "\n",
        "```python\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)\n",
        "```\n",
        "\n",
        "> We can only obtain the grad properties for the leaf nodes of the computational graph, which have `requires_grad` property set to True. For all other nodes in our graph, gradients will not be available. We can only perform gradient calculations using `backward` once on a given graph, for performance reasons. If we need to do several backward calls on the same graph, we need to pass `retain_graph=True` to the backward call.\n",
        "\n",
        "\n",
        "### Disabling gradient tracking\n",
        "\n",
        "There are reasons you might want to disable gradient tracking:\n",
        "* To mark some parameters in your neural network as frozen parameters.\n",
        "\n",
        "* To speed up computations when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient.\n",
        "\n",
        "We can stop tracking computations by surrounding our computation code with `torch.no_grad()` block:\n",
        "\n",
        "```python\n",
        "z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)  # True\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)  # False\n",
        "\n",
        "# we can achieve same result using `.detach()` method on the tensor\n",
        "z_det = z.detach()\n",
        "print(z_det.requires_grad)  # False\n",
        "```\n",
        "Autograd in PyTorch records operations in a directed acyclic graph (DAG) as you compute values forward. In this DAG, `leaves` are the `input tensors`, `roots` are the `output tensors`. Then, when you call `.backward()`, it traces this graph in reverse (from outputs back to inputs), applying the chain rule to compute gradients.\n",
        "\n",
        "In a `forward pass` autograd does two things -\n",
        "- runs the requested operation to compute a resulting tensor\n",
        "- mainitain the operation's `gradient function` in the DAG\n",
        "\n",
        "The `backward pass` kicks off when `.backward()` is called on the root. `autograd` then\n",
        "- computes gradients of each `.grad_fn`\n",
        "- accumulates them in the resepective tensor's `.grad` attribute\n",
        "- using the chain rule, propogates all the way to the leaf tensors\n",
        "\n",
        "```less\n",
        "      [ x ]      [ w ]      [ b ]\n",
        "        |          |          |\n",
        "        |          |          |\n",
        "        +----------*----------+\n",
        "                   | (mul)\n",
        "               [ x * w ]\n",
        "                   |\n",
        "                   | (+)\n",
        "                   |\n",
        "                [ xw + b ]\n",
        "                   |\n",
        "                   | (activation, e.g. ReLU)\n",
        "                   |\n",
        "               [ output ŷ ]\n",
        "                   |\n",
        "                   | (loss function)\n",
        "                   |\n",
        "               [ loss L ]\n",
        "```\n",
        "- **Forward Pass**: moves from top to bottom (inputs → output → loss).\n",
        "- **Backward Pass**: triggered by `.backward()`, moves from loss back to inputs.\n",
        "- Each edge is a **Function node** (e.g., add, multiply, relu) that knows how to compute its local gradient.\n"
      ],
      "metadata": {
        "id": "a9jNH7b7pLRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define Architecture\n",
        "input_size = 3\n",
        "hidden_size = 4\n",
        "output_size = 1\n",
        "\n",
        "# random input\n",
        "x = torch.randn(1, input_size)\n",
        "\n",
        "# Parameters(manually initialised with gradients)\n",
        "W1 = torch.randn(input_size, hidden_size, requires_grad=True)\n",
        "b1 = torch.randn(hidden_size, requires_grad=True)\n",
        "\n",
        "W2 = torch.randn(hidden_size, output_size, requires_grad=True)\n",
        "b2 = torch.randn(output_size, requires_grad=True)\n",
        "\n",
        "# Define a forward pass\n",
        "z1 = x @ W1 + b1       # Linear Layer 1\n",
        "a1 = torch.relu(z1)    # activation\n",
        "\n",
        "z2 = a1 @ W2 + b2      # Linear Layer 2\n",
        "y_pred = z2            # Output\n",
        "\n",
        "# Define a loss func\n",
        "y_true = torch.randn(1, output_size)\n",
        "loss = ((y_pred - y_true) ** 2)       # MSE Loss\n",
        "\n",
        "# backprop\n",
        "loss.backward()\n",
        "\n",
        "# manual weight update\n",
        "learning_rate = 1e-2\n",
        "\n",
        "with torch.no_grad():\n",
        "    W1 -= learning_rate * W1.grad\n",
        "    b1 -= learning_rate * b1.grad\n",
        "    W2 -= learning_rate * W2.grad\n",
        "    b2 -= learning_rate * b2.grad\n",
        "\n",
        "    # Don’t forget to zero the gradients\n",
        "    W1.grad.zero_()\n",
        "    b1.grad.zero_()\n",
        "    W2.grad.zero_()\n",
        "    b2.grad.zero_()"
      ],
      "metadata": {
        "id": "CtKSzJAMrl5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lens 2: Abstractions - From Layers to Modules\n",
        "- [x] Use `torch.nn.Module` to create a basic neural network\n",
        "- [x] Compose layers using nn.Sequential and custom modules\n",
        "- [x] Explore Parameters Management (.parameters(), .state_dict())\n",
        "\n",
        "**GOAL**: Understand how to modularize models and reuse components, like LEGO bricks"
      ],
      "metadata": {
        "id": "K9G6CZuTtISN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use torch.nn.Module to create a basic neural network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MyNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "model = MyNN(3, 4, 1)\n",
        "x = torch.randn(1, 3)\n",
        "y = model(x)\n",
        "print(y.item())"
      ],
      "metadata": {
        "id": "krP4xY2HuBt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compose with nn.Sequential\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3, 4),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(4, 1)\n",
        ")\n",
        "\n",
        "# Iterate through named parameters\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.shape)\n",
        "\n",
        "print('*'*80)\n",
        "\n",
        "for param in model.parameters():\n",
        "    print(param.data)\n",
        "\n",
        "# Check and Save state_dict()\n",
        "# Print parameter dictionary\n",
        "print(model.state_dict())\n",
        "\n",
        "# Save to file\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "# Load later\n",
        "model.load_state_dict(torch.load(\"model.pth\"))"
      ],
      "metadata": {
        "id": "s1nuopVI5rVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a nn.Sequential\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Transform: flatten the image and convert to tensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1))  # Flatten 28x28 -> 784\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(784, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 10)  # 10 output classes\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(5):\n",
        "    for images, labels in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/5], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "Hc4m06Xd7BbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Builidng a CNN using nn.Sequential\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# MNIST with Image Transform (no flattening!)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Keeps shape as [1, 28, 28]\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# MNIST images are 28x28 grayscale, so input channels = 1\n",
        "# No flattening at the start — we preserve spatial structure for convolutions\n",
        "\n",
        "# Define CNN Model with nn.Sequential\n",
        "model = nn.Sequential(\n",
        "    # Extracts 16 local features with 3x3 receptive fields.\n",
        "    # Padding=1 keeps output spatial size consistent (28x28).\n",
        "    nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    # MaxPool(2) reduces spatial dimensions to 14x14, helping with translation invariance and reducing computation.\n",
        "    nn.MaxPool2d(2),\n",
        "\n",
        "    # Doubles feature channels from 16 → 32 to capture more abstract features.\n",
        "    # Second pooling reduces spatial size to 7x7, compacting information further.\n",
        "    nn.Conv2d(16, 32, kernel_size=3, padding = 1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),\n",
        "\n",
        "    # After flattening, we feed a 1D vector of size 1568 (32×7×7) to a fully connected layer.\n",
        "    nn.Flatten(),\n",
        "    # 128 hidden units give enough capacity without overfitting.\n",
        "    nn.Linear(32*7*7, 128),\n",
        "    nn.ReLU(),\n",
        "    # Final output is 10 logits — one for each digit class.\n",
        "    nn.Linear(128, 10)\n",
        "\n",
        ")\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "for epoch in range(5):\n",
        "    for images, labels in train_loader:\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/5], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "C1OM1eyy9lkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🧠 CNN Design Cheat Sheet: Channels vs Spatial Size\n",
        "\n",
        "## 📐 General Pattern\n",
        "\n",
        "| Layer Stage     | Channels       | Spatial Size (Example: MNIST) | Notes |\n",
        "|------------------|----------------|-------------------------------|-------|\n",
        "| Input Image      | 1              | 28×28                         | Grayscale |\n",
        "| Conv Block 1     | 16             | 28×28                         | Use padding=1 |\n",
        "| MaxPool          | 16             | 14×14                         | Halve spatial |\n",
        "| Conv Block 2     | 32             | 14×14                         | Double channels |\n",
        "| MaxPool          | 32             | 7×7                           | Halve spatial |\n",
        "| Flatten → Linear | —              | 32×7×7 = 1568                 | Ready for FC |\n",
        "| FC Hidden        | 128            | —                             | Good trade-off |\n",
        "| Output Layer     | 10             | —                             | MNIST classes |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 Why Double Channels and Halve Size?\n",
        "\n",
        "> Key Idea: As we reduce spatial detail, we increase semantic richness via channels.\n",
        "\n",
        "This is a pattern called *progressive abstraction*.\n",
        "As we go deeper, features become more abstract (e.g., from edges → shapes → digits). We require more channels to capture diverse patterns. Doubling gives exponential representation power. (Can't we quadruple?)\n",
        "\n",
        "Similtarly, halving spatial size reduces computaiton cost and allows deeper networks. It also helps with local invariance -- the network need to focus less on exact positions.\n",
        "\n",
        "- **Double Channels** → Capture more abstract patterns (depth).\n",
        "- **Halve Size** → Reduce computation & increase translational robustness.\n",
        "\n",
        "> 🔧 You can halve until spatial dimensions become too small to retain meaning (e.g., < 3x3 is usually risky for small images).\n",
        "\n",
        "---\n",
        "\n",
        "## 🧮 Why 128 Hidden Units?\n",
        "\n",
        "- It's a **hyperparameter**.\n",
        "- Works well for MNIST due to:\n",
        "  - Low task complexity\n",
        "  - Limited overfitting risk\n",
        "  - Efficient training\n",
        "\n",
        "> For more complex datasets, you may increase this to 256, 512, or beyond.\n",
        "\n",
        "---\n",
        "\n",
        "## 🛠️ Tips\n",
        "\n",
        "- Always match `Conv2D` padding to preserve size if you want clean downsampling.\n",
        "- Double filters → `16 → 32 → 64`, halve size → `28 → 14 → 7` is a classic design.\n",
        "- Use `Dropout` or `BatchNorm` between layers to improve generalization.\n",
        "\n"
      ],
      "metadata": {
        "id": "rVa8ZxjcDlL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lens 3: Execution & Training Loop - The Real Engine\n",
        "- [ ] Write a manual training loop using:\n",
        "    - [ ] optimizer.step(), loss.backward(), zero.backward()\n",
        "    - [ ] DataLoader, Dataset abstractions\n",
        "- [ ] Track Model loss and accuracy across epochs    \n",
        "- [ ] **Use GPU**: `model.to(device)`, `tensor.to(device)`\n",
        "\n",
        "**GOAL**: Know what happens per epoch, batch, and step."
      ],
      "metadata": {
        "id": "1zDPhzquuCCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 📦 Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'{device = }')\n",
        "\n",
        "# 📊 Dataset and DataLoader\n",
        "transform = transforms.ToTensor()\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# 🧠 Model\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),  # 28 → 14\n",
        "    nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),  # 14 → 7\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(32 * 7 * 7, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)\n",
        "\n",
        "# 📉 Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "bKE7OVwYvHEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "\n",
        "train_loss_hist = []\n",
        "train_acc_hist = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        # 🧠 Move to device (CPU/GPU)\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # 🔁 Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 🔄 Backpropagation\n",
        "        optimizer.zero_grad()         # 🔄 Reset gradients\n",
        "        loss.backward()               # 🧮 Compute gradients\n",
        "        optimizer.step()              # ⬆️ Update weights\n",
        "\n",
        "        # 📊 Stats\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    # 📣 End of epoch reporting\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = correct / total\n",
        "\n",
        "    train_loss_hist.append(epoch_loss)\n",
        "    train_acc_hist.append(epoch_acc)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")"
      ],
      "metadata": {
        "id": "910cpb-bzEoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss_hist, label=\"Loss\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_acc_hist, label=\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LdaAU0I13du6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Test Accuracy: {correct / total:.4f}\")"
      ],
      "metadata": {
        "id": "QAuzlb9m3kYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lens 4: Philosphy and Idioms - What's considered **PyTorchic**?\n",
        "\n",
        "- [x] Use official PyTorch Tutorials.\n",
        "- [x] Study Idioms:\n",
        "    - [x] Model Initialization and reuse\n",
        "    - [x] `TorchVision` tramsforms and dataset\n",
        "    - [x] `TorchScript` for exporting models\n",
        "- [x] Read 1 open-source PyTorch repo\n",
        "\n",
        "**GOAL**: PyTorch is designed to look like Python — if something feels unnatural, you're probably doing it wrong."
      ],
      "metadata": {
        "id": "ntZ0AR7AvHcQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Concept                               | What to Do                                                                                         | PyTorchic Tips                                                        |\n",
        "| ------------------------------------- | -------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------- |\n",
        "| **Model Initialization & Reuse**      | Create model classes with `__init__()` and `forward()` and use `model.load_state_dict()` to reuse. | Keep architecture code separate from training logic.                  |\n",
        "| **TorchVision Datasets & Transforms** | Use `torchvision.transforms.Compose` and `torchvision.datasets` like `MNIST`, `CIFAR10`.           | Always visualize your transformed data.                               |\n",
        "| **Exporting with TorchScript**        | Use `torch.jit.trace()` or `torch.jit.script()` to export trained models.                          | Choose `trace` for standard forward paths, `script` for control flow. |\n",
        "| **Saving/Loading Models**             | Use `torch.save(model.state_dict(), 'model.pt')` and `load_state_dict()`                           | Save checkpoints with metadata (epoch, loss, etc.).                   |\n",
        "| **Read Open Source Code**             | Start with [PyTorch examples](https://github.com/pytorch/examples) repo, e.g. `mnist/main.py`      | Observe how they structure training, validation, config, and logging. |\n"
      ],
      "metadata": {
        "id": "q9mGJ9Gy4m8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧠 PyTorch Best Practices — For the Thoughtful Human\n",
        "\n",
        "A reference guide of often-overlooked but essential PyTorch coding best practices. These are not enterprise-level SDLC rules, but habits that make your AI code more readable, robust, and human-friendly.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. 📦 Device Handling Early\n",
        "\n",
        "```python\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "```\n",
        "\n",
        "✅ Centralize `.to(device)` calls. Apply it once to the model and input batches.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. 🧱 Separate Model, Training, and Evaluation\n",
        "\n",
        "```python\n",
        "model = MyModel()\n",
        "train(model, train_loader)\n",
        "evaluate(model, test_loader)\n",
        "```\n",
        "\n",
        "✅ Keep your functions clean. Avoid putting training logic inside your model class.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. 🔁 Use `.train()` and `.eval()` Modes Explicitly\n",
        "\n",
        "```python\n",
        "model.train()  # Enables dropout, batchnorm\n",
        "model.eval()   # Disables them\n",
        "```\n",
        "\n",
        "✅ Critical for correct behavior, especially during inference.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. 🧼 Always Call `zero_grad()` Before Backward\n",
        "\n",
        "```python\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "```\n",
        "\n",
        "✅ Avoids gradient accumulation unless you're doing it intentionally.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. ❌ Avoid `requires_grad=True` on Input Data\n",
        "\n",
        "```python\n",
        "x = x.to(device)\n",
        "```\n",
        "\n",
        "✅ Only model parameters should require gradients, not your data.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. 🚫 Use `with torch.no_grad()` for Inference\n",
        "\n",
        "```python\n",
        "with torch.no_grad():\n",
        "    output = model(x)\n",
        "```\n",
        "\n",
        "✅ Saves memory and speeds up forward pass during eval/inference.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. 📊 Track Loss and Accuracy Across Epochs\n",
        "\n",
        "```python\n",
        "train_loss.append(loss.item())\n",
        "```\n",
        "\n",
        "✅ Simple tracking avoids \"training blind.\" Don't optimize prematurely.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. 💾 Save Only `state_dict()` for Portability\n",
        "\n",
        "```python\n",
        "torch.save(model.state_dict(), 'model.pth')\n",
        "```\n",
        "\n",
        "✅ Forward-compatible and safer than saving the entire model object.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. 🧱 Prefer `nn.Sequential` for Simple Architectures\n",
        "\n",
        "```python\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, 3, 1),\n",
        "    nn.ReLU(),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(5408, 10)\n",
        ")\n",
        "```\n",
        "\n",
        "✅ Clean and modular. Ideal for feedforward-like stacks.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. ❗ Avoid Magic Numbers — Use Named Variables\n",
        "\n",
        "```python\n",
        "hidden_dim = 128\n",
        "nn.Linear(hidden_dim, hidden_dim // 2)\n",
        "```\n",
        "\n",
        "✅ Makes your model easier to reason about and tune.\n",
        "\n",
        "---\n",
        "\n",
        "## ❤️ Bonus — PyTorchic Mindset\n",
        "\n",
        "- **Write for the reader, not the compiler.**\n",
        "- **If it feels clunky, you're probably not using the idiom.**\n",
        "- **Make things explicit. Be boring in a good way.**\n",
        "- **Revisit tutorials not to learn APIs, but to learn writing styles.**\n",
        "\n",
        "---\n",
        "\n",
        "> Use this document as your compass when things feel messy. PyTorch is designed to make the code look like the idea."
      ],
      "metadata": {
        "id": "-PvfrnuU74Fy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 💡 Mindset Shifts\n",
        "\n",
        "- Think in batches (not single examples)\n",
        "\n",
        "- Track the flow: tensor → operation → loss → gradient → update\n",
        "\n",
        "- Don't memorize APIs — learn tensor transformations"
      ],
      "metadata": {
        "id": "aQdNvX7VwZVH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "37vMRpGVwsw3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}